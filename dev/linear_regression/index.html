<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Linear regressions · MethodMoments.jl</title><meta name="title" content="Linear regressions · MethodMoments.jl"/><meta property="og:title" content="Linear regressions · MethodMoments.jl"/><meta property="twitter:title" content="Linear regressions · MethodMoments.jl"/><meta name="description" content="Documentation for MethodMoments.jl."/><meta property="og:description" content="Documentation for MethodMoments.jl."/><meta property="twitter:description" content="Documentation for MethodMoments.jl."/><script data-outdated-warner src="../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL=".."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../assets/documenter.js"></script><script src="../search_index.js"></script><script src="../siteinfo.js"></script><script src="../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><div class="docs-package-name"><span class="docs-autofit"><a href="../">MethodMoments.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../">Home</a></li><li><a class="tocitem" href="../general_gmm/">General GMM models</a></li><li class="is-active"><a class="tocitem" href>Linear regressions</a><ul class="internal"><li><a class="tocitem" href="#Minimal-example"><span>Minimal example</span></a></li><li><a class="tocitem" href="#The-univariate-regression"><span>The univariate regression</span></a></li><li><a class="tocitem" href="#Asymptotic-distributions"><span>Asymptotic distributions</span></a></li><li><a class="tocitem" href="#Running-a-regression"><span>Running a regression</span></a></li><li><a class="tocitem" href="#Fit-and-model-evaluation"><span>Fit and model evaluation</span></a></li><li><a class="tocitem" href="#Available-methods"><span>Available methods</span></a></li><li><a class="tocitem" href="#Examples"><span>Examples</span></a></li></ul></li><li><a class="tocitem" href="../api/">API</a></li><li><a class="tocitem" href="../references/">References</a></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li class="is-active"><a href>Linear regressions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Linear regressions</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/liviomaya/MethodMoments.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/liviomaya/MethodMoments.jl/blob/main/docs/src/linear_regression.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Linear-Regressions"><a class="docs-heading-anchor" href="#Linear-Regressions">Linear Regressions</a><a id="Linear-Regressions-1"></a><a class="docs-heading-anchor-permalink" href="#Linear-Regressions" title="Permalink"></a></h1><p>MethodMoments.jl offers functions to solve univariate linear regression models by ordinary least squares (OLS) or through instrumental variables (IV). The goal of the tutorial below is to quickly present these functions. Hayashi (2000) provides a detailed treatment and the technical assumptions behind the results below. </p><h2 id="Minimal-example"><a class="docs-heading-anchor" href="#Minimal-example">Minimal example</a><a id="Minimal-example-1"></a><a class="docs-heading-anchor-permalink" href="#Minimal-example" title="Permalink"></a></h2><p>We simulate the regression <span>$y_t = c + \beta_1 X_{1,t} + \beta_2 X_{2,t} + e_t$</span>. The true parameters are <span>$c=0$</span>, <span>$\beta_1 = \beta_2 = 1$</span>. First, we estimate by OLS. </p><pre><code class="language-julia hljs">using MethodMoments
using Distributions
X = randn(100, 2)
Y = X[:,1] + X[:, 2] + randn(100)
ols = reg(Y, X)
summary(ols)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">

Linear Regression Estimates
 ------------- --------- --------- --------- --------- -----
                   β        std        t      p(&gt;|t|)
 ------------- --------- --------- --------- --------- -----
  (Intercept)     0.103     0.094     1.097     0.275
  Regressor 1     0.902     0.093     9.684     0.000   ***
  Regressor 2     1.002     0.078    12.893     0.000   ***
 ------------- --------- --------- --------- --------- -----
Significance: 1% (***) 2.5% (**) 5% (*) 10% (.)

R-Squared: 0.726, Adjusted R-Squared: 0.721
Residuals. Std: 0.935, Min: -2.355, Max: 2.241
Wald [β = 0]: 269.644 (p-val: 0.0 on 2 df)
Akaike IC: 278.344, Bayesian IC: 288.764</code></pre><p>Now, we estimate coefficients by IV, adding a new instrument <span>$H_t = X_{1,t} + w_t$</span>, where <span>$w_t$</span> is a random shock.</p><p>The weighting matrix must be <span>$4 \times 4$</span> (three variable <span>$X_1, X_2, N$</span> and one constant instrument).</p><pre><code class="language-julia hljs">N = X[:, 1] + randn(100)
Z = [X N] # instruments
W = [1 0 0 0; 0 1 0 0; 0 0 1 0; 0 0 0 1] # weighting matrix
iv = reg(Y, X, Z, W)
summary(iv)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">

Linear Regression Estimates
 ------------- --------- --------- --------- --------- -----
                   β        std        t      p(&gt;|t|)
 ------------- --------- --------- --------- --------- -----
  (Intercept)     0.109     0.093     1.172     0.244
  Regressor 1     0.961     0.110     8.736     0.000   ***
  Regressor 2     0.997     0.078    12.762     0.000   ***
 ------------- --------- --------- --------- --------- -----
Significance: 1% (***) 2.5% (**) 5% (*) 10% (.)

R-Squared: 0.725, Adjusted R-Squared: 0.72
Residuals. Std: 0.937, Min: -2.309, Max: 2.181
Wald [β = 0]: 260.781 (p-val: 0.0 on 2 df)
Akaike IC: 278.733, Bayesian IC: 289.154</code></pre><h2 id="The-univariate-regression"><a class="docs-heading-anchor" href="#The-univariate-regression">The univariate regression</a><a id="The-univariate-regression-1"></a><a class="docs-heading-anchor-permalink" href="#The-univariate-regression" title="Permalink"></a></h2><p>The univariate linear model is </p><p class="math-container">\[    y_t = \beta&#39; X_t + e_t \]</p><p>where <span>$y_t$</span> is a scalar (hence &quot;univariate&quot;), <span>$X_t$</span> is a <span>$P$</span>-sized vector of explanatory variables (or covariates), <span>$\beta$</span> is the true parameter vector, and <span>$e_t$</span> is the error term with variance <code>\sigma^2</code>. Let <span>$Z_t$</span> be a <span>$M$</span>-sized vector of instruments. The moment condition we apply to estimate <span>$\beta$</span> is</p><p class="math-container">\[    E \left[ e_t Z_t \right] = 0.       
    \qquad \qquad (1)\]</p><p>Hence, there are <span>$P$</span> parameters and <span>$M$</span> moment conditions. </p><p>The IV estimator requires the following two conditions to hold.</p><ul><li><strong>Order condition</strong>: <span>$M \geq P$</span>. If <span>$M = P$</span>, the model is exactly identified. If <span>$M &gt; P$</span>, the model is overidentified. If <span>$M &lt; P$</span>, the model is underidentified and cannot be estimated, as there are multiple solutions to the sample counterpart of (1).</li><li><strong>Rank condition</strong>: Matrix <span>$E\left[ Z_t X_t&#39; \right]$</span> must be full column rank, i.e., its rank must be <span>$P$</span>.</li></ul><p>Let <span>$y_{T \times 1}$</span> be a sample of <span>$y_t$</span> with <span>$T$</span>observations,  <span>$X_{T \times P}$</span> a sample of explanatory variables (each row corresponding to one observation <span>$X_t&#39;$</span>), and <span>$Z_{T \times M}$</span> a sample of instruments. </p><p>Let <span>$\hat{W}$</span> be the weighting matrix of a GMM problem that estimates <span>$\beta$</span> by targeting moment conditions (1) (see <a href="../general_gmm/#The-GMM-setup">The GMM setup</a>). The estimator of <span>$\beta$</span> that solves that problem, or the GMM estimator is </p><p class="math-container">\[    \hat{\beta} =  (X&#39;Z \, \hat{W} \, Z&#39;X)^{-1} \, X&#39;Z \,  \hat{W} \, Z&#39;y \qquad \qquad (2)\]</p><p>If the model is exactly identified, (<span>$M = P$</span>), (2) becomes the <strong>instrumental variables</strong> (IV) estimator:</p><p class="math-container">\[    \hat{\beta}_{IV} = (Z&#39;X)^{-1} Z&#39;y. \qquad \qquad (3)\]</p><p>If covariates are themselves the instruments (<span>$Z_t = X_t$</span>), (2) becomes the <strong>ordinary least squares</strong> (OLS) estimator:</p><p class="math-container">\[    \hat{\beta}_{OLS} = (X&#39;X)^{-1} X&#39;y. \qquad \qquad (4)\]</p><p>Both the IV and the OLS estimators perfectly replicate moment conditions (1), in sample. </p><p>If the weighting matrix is <span>$\hat{W} = (Z&#39;Z)^{-1}$</span>, (2) becomes the <strong>two-stage least squares</strong> (2SLS) estimator:</p><p class="math-container">\[    \hat{\beta}_{2SLS} =  [(PX)&#39; (PX)]^{-1} \, (PX)&#39; y \qquad \qquad (5)\]</p><p>where <span>$P = Z (Z&#39;Z)^{-1} Z&#39;$</span> is the projection matrix of <span>$Z$</span>. The 2SLS estimator (5) is the OLS estimator of a regression of <span>$y$</span> on the OLS fit of <span>$X$</span> on <span>$Z$</span>.</p><h2 id="Asymptotic-distributions"><a class="docs-heading-anchor" href="#Asymptotic-distributions">Asymptotic distributions</a><a id="Asymptotic-distributions-1"></a><a class="docs-heading-anchor-permalink" href="#Asymptotic-distributions" title="Permalink"></a></h2><p>Let <span>$W = \hat{W}$</span> or its probability limit if <span>$\hat{W}$</span> is a function of the sample.</p><p>Let <span>$\hat{S}$</span> be a consistent estimator of the long-run variance matrix <span>$S = \lim_{T \rightarrow \infty} T \ E \left[ \hat{g}(\beta) \hat{g}(\beta)&#39; \right]$</span> (where <span>$\hat{g}(b) \equiv T^{-1} \sum_{t=1}^T (y_t - b&#39;X_t) Z_t$</span> is the empirical average of the moment function). See section <a href="../general_gmm/#Long-run-variance">Long-run variance</a> for more details.</p><p>Let <span>$D = -Z&#39; X / T$</span>, which consistently estimates <span>$D = -E[Z_t X_t&#39;]$</span>.</p><p><strong>Parameter Distribution</strong>: <span>$\hat{\beta} \overset{p}{\to} \beta$</span> and </p><p class="math-container">\[    \sqrt{T} (\hat{\beta} - \beta) \to \mathcal{N}(0, V_\beta) 
    \qquad \text{where} \qquad
    V_\beta = (D&#39; W D)^{-1} D&#39; W S W&#39; D (D&#39; W&#39; D)^{-1}\]</p><p>If <span>$W = S^{-1}$</span> is the optimal weighting matrix, then <span>$V_\beta = (D&#39; S^{-1} D)^{-1}$</span>. </p><p><strong>Moments Distribution</strong>: <span>$\hat{g}(\hat{\beta}) \overset{p}{\to} 0$</span> and </p><p class="math-container">\[    \sqrt{T} \hat{g}(\hat{\beta}) \to \mathcal{N}(0, V_g) 
    \qquad \text{where} \qquad
    V_g = (I - D (D&#39; W D)^{-1} D&#39; W) S (I - D (D&#39; W D)^{-1} D&#39; W)&#39;\]</p><p>If the model is exactly identified, <span>$V_g = 0_{M \times M}$</span>.</p><p>If <span>$W = S^{-1}$</span> is the optimal weighting matrix, then <span>$V_g = S - D(D&#39; S^{-1}D)^{-1} D&#39;$</span>. </p><p>Note the difference between <span>$S$</span> and <span>$V_g$</span>: the former is the asymptotic variance of <span>$\hat{g}(\beta)$</span>; the latter is that of <span>$\hat{g}(\hat{\beta})$</span>. <span>$V_g$</span> is typically a singular matrix.</p><p>The Sargan-Hansen test for overidentified restrictions, or J test, uses the statistic <span>$\hat{J} = \hat{g}(\hat{\theta})&#39; \hat{W} \hat{g}(\hat{\theta})$</span>. If <span>$\hat{W} = \hat{S}^{-1}$</span>, then </p><p class="math-container">\[    T \ \hat{J} \rightarrow \chi^2 (M-P).\]</p><p><strong>Estimators</strong>: The (consistent) estimators <span>$\hat{V}_\beta$</span> of <span>$V_\beta$</span> and <span>$\hat{V}_g$</span> of <span>$V_g$</span> computed by GMM.jl simply replace <span>$\hat{W}$</span>, <span>$\hat{D}$</span> and <span>$\hat{S}$</span> in the corresponding formulas.</p><h2 id="Running-a-regression"><a class="docs-heading-anchor" href="#Running-a-regression">Running a regression</a><a id="Running-a-regression-1"></a><a class="docs-heading-anchor-permalink" href="#Running-a-regression" title="Permalink"></a></h2><p>Use the constructor function <a href="../api/#MethodMoments.reg"><code>reg</code></a> to build an object of type <a href="../api/#MethodMoments.Regression"><code>Regression</code></a>, further discussed below. Function <a href="../api/#MethodMoments.reg"><code>reg</code></a> takes up to four positional arguments: the output vector <code>Y</code>, the regressor array <code>X</code>, the instruments array <code>Z</code> and the weighting matrix <code>W</code>.</p><pre><code class="language-julia hljs">reg(Y, X, Z, W; intercept=true)</code></pre><p><code>Y</code> should be an <code>AbstractVector</code>. <code>X</code> and <code>Z</code> should be <code>AbstractVecOrMat</code>. <code>Y</code>, <code>X</code> and <code>Z</code> must present observations in rows, variables in columns. <code>W</code> must be a square, positive-definite <code>AbstractMatrix</code>. Weighting matrix must be square, of size <code>size(Z, 2) + intercept</code>. You can omit <code>Z</code> and <code>W</code>.</p><ul><li>To estimate by <strong>OLS</strong>, run <code>reg(Y, X)</code>.</li><li>To estimate by <strong>IV</strong>, run <code>reg(Y, X, Z, W)</code>.</li><li>If <code>W</code> is omitted, it defaults to the identity.</li><li>If <code>intercept = true</code>, <code>reg</code> adds a constant <em>both</em> as a regressor and as an instrument.</li></ul><p><a href="../api/#MethodMoments.Regression"><code>Regression</code></a> objects are not mutable. Its fields are:</p><ul><li><code>G::GMM</code>: <a href="../api/#MethodMoments.GMM"><code>GMM</code></a> object corresponding to the orthogonality conditions (1)</li><li><code>Y::Vector</code>: output vector <span>$Y$</span></li><li><code>X::Matrix</code>: right-hand side variables <span>$X$</span></li><li><code>Z::Matrix</code>: instruments <span>$Z$</span></li><li><code>W::Matrix</code>: weighting matrix <span>$W$</span></li><li><code>intercept::Bool</code>: <code>true</code> if constant added</li><li><code>nobs::Int</code>: number of observations <span>$T$</span></li><li><code>nins::Int</code>: number of instruments (including constant) <span>$M$</span></li><li><code>nrhs::Int</code>: number of right-hand side variables (including intercept) <span>$P$</span></li></ul><h2 id="Fit-and-model-evaluation"><a class="docs-heading-anchor" href="#Fit-and-model-evaluation">Fit and model evaluation</a><a id="Fit-and-model-evaluation-1"></a><a class="docs-heading-anchor-permalink" href="#Fit-and-model-evaluation" title="Permalink"></a></h2><p>Besides estimators and their asymptotic distribution, GMM.jl computes some additional statistics related to model fit and evaluation.</p><p>The sample error is <span>$e_t = y_t - \hat{\beta}&#39; X_t$</span>. The MLE estimate of the <strong>error variance</strong> is </p><p class="math-container">\[    \hat{\sigma}^2_e = \frac{1}{T} \sum_{t=1}^\infty e_t^2\]</p><p>The coefficient of determination, or <strong>R-squared</strong>, is</p><p class="math-container">\[    R^2 = 1 - \frac{\hat{\sigma}^2_e}{\hat{\sigma}_y^2}\]</p><p>where <span>$\hat{\sigma}_y^2 = T^{-1} \sum_{t=1}^T (y_t - \bar{y})^2$</span> estimates the unconditional variance of <span>$y_t$</span> (here, <span>$\bar{y}$</span> is the sample average of <span>$y_t$</span>). The <strong>adjusted R-squared</strong> is</p><p class="math-container">\[    R^2_{adj} = 1 - (1 - R^2) \times \frac{T-1}{T-P}\]</p><p>Assuming errors are Gaussian, the estimated <strong>log-likelihood</strong> of the model is</p><p class="math-container">\[    LLH = -\frac{T}{2} \log(2 \pi) - \frac{T}{2} \log( \hat{\sigma}^2 ) - \frac{\sum_{t=1}^T e_t^2}{ 2 \hat{\sigma}^2}\]</p><p>The <strong>Akaike information criterion</strong> is</p><p class="math-container">\[    AIC = 2 \times P - 2 \times LLH.\]</p><p>The <strong>Bayesian information criterion</strong> is </p><p class="math-container">\[    BIC = \log(T) \times P - 2 \times LLH.\]</p><p><span>$AIC$</span> and <span>$BIC$</span> are often used for model selection. </p><h2 id="Available-methods"><a class="docs-heading-anchor" href="#Available-methods">Available methods</a><a id="Available-methods-1"></a><a class="docs-heading-anchor-permalink" href="#Available-methods" title="Permalink"></a></h2><p>The methods below always take a <code>Regression</code> object <code>re</code> as argument, and never change <code>re</code> in place. Many of these methods derive from the GMM methods applied to the <code>GMM</code> object <code>re.G</code>. See <a href="../api/">API documentation</a> for details.</p><ul><li><code>intercept(re)</code>: get intercept.</li><li><code>coef(re)</code>: get estimated coefficients (excluding intercept).</li><li><code>vcov(re; lags=0)</code>: return new <code>Regression</code> object storing the Newey-West estimator of the long-run covariance matrix <span>$\hat{S}$</span>, with selected number of <code>lags</code>, in its GMM field  (see <a href="../general_gmm/#Long-run-variance">Long-run variance</a>). Use this method to change estimates of coefficients&#39; variance and covariance. When <code>lags=0</code>, we get White standard errors.</li><li><code>cov(re)</code>: get covariance matrix of coefficients <span>$V_\beta / T$</span>, excluding the intercept term. (If you are interested in the variance of the intercept term, run <code>cov(re.G)</code>). </li><li><code>var(re)</code>: get element-by-element variance of coefficients, excluding the intercept term. Use <code>std(re)</code> for standard errors.</li><li><code>cor(re)</code>: get correlation matrix of coefficients, excluding the intercept term</li><li><code>fit(re, X)</code>: fit <span>$X \beta$</span> using estimated coefficients. If <code>X</code> is omitted, apply to sample used for estimation <code>re.X</code>.</li><li><code>er(re)</code>: sample of fitted errors (dim 1: moments, dim 2: parameters).</li><li><code>er_var(re)</code>: get maximum likelihood estimate of error variance <span>$\sigma^2$</span> (no correction for lost degrees of freedom). Use <code>er_std</code> to get standard deviation <span>$\sigma$</span>.</li><li><code>r2(re, adjust=false)</code>: get coefficient of determination <span>$R^2$</span>, adjusted or not.</li><li><code>llh(re)</code>: get log-likelihood <span>$LLH$</span>.</li><li><code>aic(re)</code>: get Akaike information criterion <span>$AIC$</span>.</li><li><code>bic(re)</code>: get Bayesian information criterion <span>$BIC$</span>.</li><li><code>summary(re)</code>: print summary of results.</li><li><code>wald(re; R, r, subset)</code>: print result of the Wald test on the null <span>$R \beta_{sub} = r$</span>, where <span>$\beta_{sub}$</span> is a subset of <span>$\beta$</span> corresponding to <code>β[subset]</code>, <strong>excluding the intercept</strong>.</li></ul><div class="admonition is-info"><header class="admonition-header">Info</header><div class="admonition-body"><p>Calling <code>vcov(re)</code> changes the long-term covariance matrix stored in the GMM object <code>re.G</code>, so you don&#39;t need to change it manually.</p></div></div><h2 id="Examples"><a class="docs-heading-anchor" href="#Examples">Examples</a><a id="Examples-1"></a><a class="docs-heading-anchor-permalink" href="#Examples" title="Permalink"></a></h2><p>We return to the example at the top of the page. Object <code>ols</code> stores the results for the regression <span>$y_t = c + \beta_1 X_{1,t} + \beta_2 X_{2,t} + e_t$</span>. For instance, coefficients and White standard deviations:</p><pre><code class="language-julia hljs">println(&quot;OLS estimates: $(coef(ols))&quot;)
println(&quot;(standard deviation): $(std(ols))&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">OLS estimates: [0.901993983209317, 1.0020331509275473]
(standard deviation): [0.09314649440100164, 0.07771693123286746]</code></pre><p>To change variance estimates to the Newey-West formula with three lags:</p><pre><code class="language-julia hljs">ols = vcov(ols, lags=3)
println(&quot;(standard deviation): $(std(ols))&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">(standard deviation): [0.09034606878477848, 0.06688029586667034]</code></pre><p>Intercept:</p><pre><code class="language-julia hljs">println(&quot;Intercept: $(intercept(ols))&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Intercept: 0.10256904461467964</code></pre><p>Field <code>ols.G</code> stores the <code>GMM</code> object referring to the orthogonality conditions that define OLS estimate. It contains all relevant data for the problem. For instance, <code>ols.G.params</code> returns coefficient estimates, including the intercept. </p><pre><code class="language-julia hljs">println(&quot;All params: $(ols.G.params)&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">All params: [0.10256904461467964, 0.901993983209317, 1.0020331509275473]</code></pre><p>Therefore, to access the standard deviation of the intercept term, you can call <code>std(ols.G)[1]</code>. (When the model involves an intercept, it is always ordered first in the GMM problem.)</p><p>Given an array of right-hand side regressors, we can fit it using estimated coefficients: </p><pre><code class="language-julia hljs">new_X = randn(50, 2) # no need to add intercept
fit(ols, new_X)</code></pre><p>Omitting the second argument, <code>fit(ols)</code> returns the fit of the sample <span>$X$</span> stored in the regression object (used for estimation) <code>ols.X</code>.</p><p>To access the sample of fitted errors, run <code>er(ols)</code>. The estimate of the variance of these errors <span>$\sigma^2$</span> is:</p><pre><code class="language-julia hljs">println(&quot;Error variance: $(er_var(ols))&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Error variance: 0.87420371362718</code></pre><p>We can access the following model evaluation metrics:</p><pre><code class="language-julia hljs">println(&quot;R-squared: $(r2(ols))&quot;)
println(&quot;Akaike criterion: $(aic(ols))&quot;)
println(&quot;Bayesian criterion: $(bic(ols))&quot;)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">R-squared: 0.7264733274996906
Akaike criterion: 278.3435217876661
Bayesian criterion: 288.7642025316184</code></pre><p>To compute a summary of the model, use method <code>summary</code>, as in the top of this page. </p><p>Finally, we can perform a Wald test of the null hypothesis <span>$R \beta_{subset} = r$</span>. In the example below, we test <span>$\beta_1 + \beta_2 = 2$</span> and <span>$\beta_1 = 1$</span>.  </p><pre><code class="language-julia hljs">R = [1 1; 1 0]
r = [2, 1]
subset = [1, 2]
wald(ols, R, r, subset)</code></pre><pre class="documenter-example-output"><code class="nohighlight hljs ansi">Wald: 1.188 (p-val: 0.552 on 2 df)</code></pre><p>Keyword argument <code>subset</code> of <code>wald</code> does not consider the intercept. Therefore, <code>subset = [1]</code> would refer to the first angular coefficient <code>coef(ols)[1]</code>, not <code>intercept(ols)</code>. </p><p>If your test involves the intercept term, use method <code>wald</code> applied to underlying GMM object (which includes the intercept term): <code>wald(ols.G, R, r, subset)</code>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../general_gmm/">« General GMM models</a><a class="docs-footer-nextpage" href="../api/">API »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.8.0 on <span class="colophon-date" title="Saturday 25 January 2025 22:15">Saturday 25 January 2025</span>. Using Julia version 1.11.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
