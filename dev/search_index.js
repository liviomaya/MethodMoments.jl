var documenterSearchIndex = {"docs":
[{"location":"references/#References","page":"References","title":"References","text":"","category":"section"},{"location":"references/","page":"References","title":"References","text":"Cochrane, J. H. (2009). Asset pricing: Revised edition. Princeton University Press.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Hamilton, J. D. (1994). Time series analysis. Princeton University Press.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Hayashi, F. (2000). Econometrics. Princeton University Press.","category":"page"},{"location":"references/","page":"References","title":"References","text":"Newey, W. K., & West, K. D. (1987). A Simple, Positive Semi-Definite, Heteroskedasticity and Autocorrelation Consistent Covariance Matrix. Econometrica, 55(3), 703–708.","category":"page"},{"location":"references/","page":"References","title":"References","text":"White, H. (1980). A Heteroskedasticity-Consistent Covariance Matrix Estimator and a Direct Test for Heteroskedasticity. Econometrica, 48(4), 817–838.","category":"page"},{"location":"api/#API-Documentation","page":"API","title":"API Documentation","text":"","category":"section"},{"location":"api/#Table-of-Contents","page":"API","title":"Table of Contents","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]\nDepth = 3","category":"page"},{"location":"api/#Index","page":"API","title":"Index","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Pages = [\"api.md\"]","category":"page"},{"location":"api/#Public-Interface","page":"API","title":"Public Interface","text":"","category":"section"},{"location":"api/#Types","page":"API","title":"Types","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"GMM\nRegression","category":"page"},{"location":"api/#MethodMoments.GMM","page":"API","title":"MethodMoments.GMM","text":"Type GMM stores data related to the generalized method of moments corresponding to the condition E[f(θ)] = 0. The GMM estimator solves\n\nMin g(θ)' W g(θ)\n\nwhere g(θ) is the sample average of f(θ). \n\nUse function gmm to define an instance of GMM, and then optimize to solve the minimization.\n\nFields\n\nfunc::Function: sample function f\nparams::Vector: parameter vector θ\nweight::Matrix: weighting matrix W\nlongcov::Matrix: estimate for long-run covariance matrix (see vcov)\ntwostep::Bool: true if GMM problem was reoptimized using efficient choice of W (see reoptimize)\nnobs::Int: sample size\nnmom::Int: number of moments\nnpar::Int: number of parameters\n\n\n\n\n\n","category":"type"},{"location":"api/#MethodMoments.Regression","page":"API","title":"MethodMoments.Regression","text":"Type Regression stores data related to the estimation of the linear regression\n\nY = X β + e\n\nthrough independent instruments Z. In the ordinary least squares (OLS) case, Z = X.\n\nUse function reg to define an instance of Regression.\n\nFields\n\nG::GMM: solution to GMM problem on corresponding to orthogonality conditions\nY::Vector: sample of dependent variable\nX::Matrix: sample of right-hand side variables\nZ::Matrix: sample of instrumental variables\nW::Matrix: weighting matrix\nintercept::Bool: true if model estimated with an intercept\nnobs::Int: sample size\nnins::Int: number of instrumental variables\nnrhs::Int: number of right-hand side variables\n\n\n\n\n\n","category":"type"},{"location":"api/#Functions","page":"API","title":"Functions","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"gmm\nsample\nmoments\nvcov\njacobian\noptimize\nreoptimize\ncov\nvar\nstd\ncor\nmomcov\nsummary\nwald\nreg\ncoef\nintercept\nfit\ner\ner_var\ner_std\nr2\nllh\naic\nbic","category":"page"},{"location":"api/#MethodMoments.gmm","page":"API","title":"MethodMoments.gmm","text":"gmm(f, θ, W=I)\n\nCreate a GMM object for the generalized method of moments problem Min g(θ)' W g(θ) where g(θ) is the sample average of f(θ). See GMM. \n\nTo solve the minimization, use optimize.\n\nArguments\n\nf::Function: sample of moments function (output must be Vector or Matrix).\nθ::Vector: parameter vector.\nW::Matrix: weighting matrix (default = identity).\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsBase.sample","page":"API","title":"StatsBase.sample","text":"sample(G::GMM)\n\nCompute sample of moments f(θ) (dim 1: observations, dim 2: moments). See documentation of type GMM for notation.  \n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.moments","page":"API","title":"MethodMoments.moments","text":"moments(G::GMM)\n\nCompute vector of empirical moments g(θ), which is the sample average of f(θ). See documentation of type GMM for notation.  \n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.vcov","page":"API","title":"MethodMoments.vcov","text":"vcov(G::GMM; lags=0)\n\nReturns a new GMM object, updating the estimate of the long-run covariance matrix. The new estimate is the heteroskedasticity and autocorrelation consistent (HAC) Newey-West estimator with specified lags. When lag = 0, the Newey-West reduces to the Huber-White estimator. \n\n\n\n\n\nvcov(R::Regresion; lags=0)\n\nReturns a new Regression object, updating the estimate of the long-run covariance matrix. The new estimate is the heteroskedasticity and autocorrelation consistent (HAC) Newey-West estimator with specified lags. When lag = 0, the Newey-West reduces to the Huber-White estimator. \n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.jacobian","page":"API","title":"MethodMoments.jacobian","text":"jacobian(G::GMM)\n\nCompute the jacobian matrix ∂g(θ)/∂θ by finite differences (dim 1: moments, dim 2: paramaters). See documentation of type GMM for notation.\n\n\n\n\n\n","category":"function"},{"location":"api/#Optim.optimize","page":"API","title":"Optim.optimize","text":"optimize(G::GMM; <kwargs>)\n\nSolve the minimization problem Min g(θ)' W g(θ). See documentation of type GMM for notation. \n\nThe optimization uses the optimize function from the Optim package, passing over all keyword arguments (e.g. method=Optim.BFGS()).\n\noptimize returns a GMM object storing the parameter vector θ that solves the minimization problem. The long-run covariance matrix is not replaced (use vcov).\n\nTo solve the minimization problem again using the feasible efficient weighting matrix, use reoptimize.\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.reoptimize","page":"API","title":"MethodMoments.reoptimize","text":"reoptimize(G::GMM; <kwargs>)\n\nSolve the minimization problem Min g(θ)' S⁻¹ g(θ), where S is the estimated long-run covariance matrix stored in a. See documentation of type GMM for notation. \n\nThe optimization uses the optimize function from the Optim package, passing over all keyword arguments (e.g. method=Optim.BFGS()).\n\noptimize returns a GMM object storing the parameter vector θ that solves the minimization problem. The long-run covariance matrix is not re-estimated (use vcov).\n\n\n\n\n\n","category":"function"},{"location":"api/#Statistics.cov","page":"API","title":"Statistics.cov","text":"cov(G::GMM)\n\nCompute the covariance matrix of optimized parameter vector θ. See documentation of type GMM for notation. \n\n\n\n\n\ncov(R::Regression)\n\nCompute the covariance matrix of linear coefficients β, excluding the intercept term. Run cov(o.G) to include intercept. See documentation of type Regression for notation. \n\n\n\n\n\n","category":"function"},{"location":"api/#Statistics.var","page":"API","title":"Statistics.var","text":"var(G::GMM)\n\nCompute the (element-by-element) variance of optimized parameter vector θ. See documentation of type GMM for notation. \n\n\n\n\n\nvar(R::Regression)\n\nCompute the (element-by-element) variance of linear coefficients β, excluding the intercept term. See documentation of type Regression for notation. \n\n\n\n\n\n","category":"function"},{"location":"api/#Statistics.std","page":"API","title":"Statistics.std","text":"std(G::GMM)\n\nCompute the (element-by-element) standard deviation of optimized parameter vector θ. See documentation of type GMM for notation. \n\n\n\n\n\nvar(R::Regression)\n\nCompute the (element-by-element) standard deviation of linear coefficients β, excluding the intercept term. See documentation of type Regression for notation. \n\n\n\n\n\n","category":"function"},{"location":"api/#Statistics.cor","page":"API","title":"Statistics.cor","text":"cor(G::GMM)\n\nCompute the correlation matrix of optimized parameter vector θ. See documentation of type GMM for notation. \n\n\n\n\n\ncov(R::Regression)\n\nCompute the correlation matrix of linear coefficients β, excluding the intercept term. See documentation of type Regression for notation. \n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.momcov","page":"API","title":"MethodMoments.momcov","text":"momcov(G::GMM)\n\nCompute the (singular) covariance matrix of minimized moments g(θ). See documentation of type GMM for notation. \n\n\n\n\n\n","category":"function"},{"location":"api/#Base.summary","page":"API","title":"Base.summary","text":"summary(G::GMM)\n\nPrint summary of the GMM problem.\n\n\n\n\n\nsummary(R::Regression)\n\nPrint summary of the linear regression.\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.wald","page":"API","title":"MethodMoments.wald","text":"wald(G::GMM; R=I, r=0, subset)\n\nPrint result of the Wald test of the null R θ[subset] = r. See documentation of type GMM for notation. \n\nIf omitted, subset defaults to all parameters 1:G.npar. R defaults to the identity, and r defaults to a vector of zeros.\n\n\n\n\n\nwald(re::Regression; R=I, r=0, subset)\n\nPrint result of the Wald test of the null R β[subset] = r, where β excludes the intercept. See documentation of type Regression for notation.\n\nIf omitted, subset defaults to all parameters (excluding the intercept). R defaults to the identity, and r defaults to a vector of zeros.\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.reg","page":"API","title":"MethodMoments.reg","text":"reg(Y, X, Z, W; intercept)\nreg(Y, X, Z)\nreg(Y, X)\n\nCreate a Regression object with the instrumental variable (IV) estimator of β in the linear regression Y = X β + e.\n\nIf weighting matrix W is omitted, use identity. \n\nIf sample of instruments Z is ommitted, estimate β by ordinary least squares (OLS).\n\nArguments\n\nY::Vector: sample of dependent variable\nX::Matrix: sample of right-hand side variables\nZ::Matrix: sample of instrumental variables\nW::Matrix: weighting matrix\nintercept::Bool: true to add intercept on both X and Z (default: true)\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.coef","page":"API","title":"MethodMoments.coef","text":"coef(R::Regression)\n\nReturn coefficient vector β, excluding intercept. To get intercept, use intercept. See documentation of type Regression for notation.\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.intercept","page":"API","title":"MethodMoments.intercept","text":"intercept(R::Regression)\n\nReturn intercept term.\n\n\n\n\n\n","category":"function"},{"location":"api/#StatsAPI.fit","page":"API","title":"StatsAPI.fit","text":"fit(R::Regression, X::VecOrMat)\nfit(R::Regression)\n\nCompute fit X β. See documentation of type Regression for notation. If X is not provided, use regressors stored in R (used for estimation).\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.er","page":"API","title":"MethodMoments.er","text":"er(R::Regression, Y::Vector, X::VecOrMat)\ner(R::Regression)\n\nCompute sample errors Y - X β. See documentation of type Regression for notation. If X and Y are not provided, use those stored in R (used for estimation).\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.er_var","page":"API","title":"MethodMoments.er_var","text":"er_var(R::Regression)\n\nReturn the estimated variance of the error term. No adjustment for degrees of freedom lost.\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.er_std","page":"API","title":"MethodMoments.er_std","text":"er_std(R::Regression)\n\nReturn the estimated standard deviation of the error term. No adjustment for degrees of freedom lost.\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.r2","page":"API","title":"MethodMoments.r2","text":"r2(R::Regression; adjust::Bool=false)\n\nReturn the coefficient of determination R², adjusted or not for model complexity.\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.llh","page":"API","title":"MethodMoments.llh","text":"llh(R::Regression)\n\nReturn the log-likelihood of the estimated model.\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.aic","page":"API","title":"MethodMoments.aic","text":"aic(R::Regression)\n\nReturn the Akaike Information Criterion.\n\n\n\n\n\n","category":"function"},{"location":"api/#MethodMoments.bic","page":"API","title":"MethodMoments.bic","text":"bic(R::Regression)\n\nReturn the Bayesian Information Criterion.\n\n\n\n\n\n","category":"function"},{"location":"general_gmm/#General-GMM-Models","page":"General GMM models","title":"General GMM Models","text":"","category":"section"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"The objective of this introduction is to quickly get you up and running with the package. For a comprehensive treatment of the generalized method of moments (GMM), see Hamilton (1994) or Hayashi (2000). For an introduction in the context of asset pricing, see Cochrane (2009).","category":"page"},{"location":"general_gmm/#Minimal-example","page":"General GMM models","title":"Minimal example","text":"","category":"section"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"If you are familiar with GMM problems, you might prefer to grasp basic usage of the package through a simple example. Here, we estimate the shape parameter of an Exponential distribution using the moment conditions EX - theta = 0 and EX^2 - 2 theta^2 = 0. With a random sample, we define the moment function (dim 1: observations, dim 2: moments). ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"using MethodMoments \nusing Distributions, Optim\nimport Random # hide\nRandom.seed!(1) # hide\nx = rand(Exponential(), 1000) # Exponential sample (θ = 1)\nf(θ) = [(x .- θ[1]) (x .^ 2 .- 2 * θ[1]^2)] \nnothing # hide","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Start by building a GMM object through the gmm function. ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"# Define GMM object\nweight = [1 0; 0 1]  \nθ_guess = [2.0]\nG = gmm(f, θ_guess, weight)\nnothing # hide","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"If you already computed the GMM estimate of theta and passed it onto gmm, you can stop here. Otherwise:","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"# Solve GMM (exogenous weighting)\nG = optimize(G)\nnothing # hide","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"To solve the GMM problem again using a consistent estimate of the efficient weighting matrix:","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"# Solve GMM (optimal weighting)\nG = vcov(G) # update long-run covariance matrix using first-stage estimates\nGO = reoptimize(G)\nsummary(GO)","category":"page"},{"location":"general_gmm/#The-GMM-setup","page":"General GMM models","title":"The GMM setup","text":"","category":"section"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Let X_1 X_2 dots X_T be a sample of vector-valued random variables and theta_0 in mathbbR^P the true yet unknown parameter vector. Given a function f(x theta) in mathbbR^M, with M geq P,  the moment condition g(theta_0) equiv Ef(X theta_0) = 0 holds.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Fixing the sample, let hatg(theta) = sum_t=1^T f(X_t theta) T be the sample mean of f. Given a positive-definite weighting matrix hatW, the basic GMM problem solves","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"    textMin_theta quad hatg(theta)  hatW  hatg(theta) qquad (1)","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"The GMM estimator hattheta is the parameter vector that solves (1). ","category":"page"},{"location":"general_gmm/#Asymptotic-distributions","page":"General GMM models","title":"Asymptotic distributions","text":"","category":"section"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"The following results of consistency and asymptotic distributions assume regularity conditions described in the references.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Let W = hatW or its probability limit if hatW is a function of the sample.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Let hatS be a consistent estimator of S = lim_T rightarrow infty T  E left hatg(theta_0) hatg(theta_0) right, the asymptotic covariance matrix of sample moments hatg(theta_0) (see Long-run variance).","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Let hatD = partial hatg(hattheta)partial theta, which consistently estimates the Jacobian D = partial g(theta_0)partial theta (see Computing the Jacobian).","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Parameter Distribution: hattheta oversetpto theta_0 and ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"    sqrtT (hattheta - theta_0) to mathcalN(0 V_theta) \n    qquad textwhere qquad\n    V_theta = (D W D)^-1 D W S W D (D W D)^-1","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"If W = S^-1 is the optimal weighting matrix, then V_theta = (D S^-1 D)^-1. ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Moments Distribution: hatg(hattheta) oversetpto 0 and ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"    sqrtT hatg(hattheta) to mathcalN(0 V_g) \n    qquad textwhere qquad\n    V_g = (I - D (D W D)^-1 D W) S (I - D (D W D)^-1 D W)","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"If W = S^-1 is the optimal weighting matrix, then V_g = S - D(D S^-1D)^-1 D.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Note the difference between S and V_g: the former is the asymptotic variance of hatg(theta_0); the latter is that of hatg(hattheta). V_g is typically a singular matrix.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"The Sargan-Hansen test for overidentified restrictions, or J test, uses the statistic hatJ = hatg(hattheta) hatW hatg(hattheta). If hatW = hatS^-1, then ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"    T  hatJ rightarrow chi^2 (M-P)","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Estimators: The (consistent) estimators hatV_theta of V_theta and hatV_g of V_g computed by MethodMoments.jl simply replace hatW, hatD and hatS in the corresponding formulas.","category":"page"},{"location":"general_gmm/#Defining-the-GMM-object","page":"General GMM models","title":"Defining the GMM object","text":"","category":"section"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"To start using the package, use the constructor function gmm, which takes three arguments: the sample function f (f), a parameter vector θ (theta), and a weighting matrix W (hatW). ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"G = gmm(f, θ, W)","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"θ should be a numerical AbstractVector, not a scalar. \nf must take a single argument. f(θ) must be a numerical Matrix or Vector, with observations in rows and moments in columns. \nW must be a square, positive-definite AbstractMatrix. If omitted, W = identity.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"The output of gmm is an object of type GMM, discussed below.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Note that gmm does not solve minimization problem (1) by itself. To perform a numerical optimization, use methods optimize and reoptimize after defining the GMM object.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"tip: Tip\nThis version of MethodMoments.jl does not support lower and upper bounds for theta. To impose such constraints, you can have f return a very large value. (But be careful: If optimize uses gradient or Hessian-based optimization algorithms, f should not return Inf or -Inf.) ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"GMM objects are not mutable. They store only the essential data related to the problem. Its fields are:","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"func::Function: sample function f\nparams::Vector: parameter vector hattheta\nweight::Matrix: weighting matrix hatW\nlongcov::Matrix: long-run covariance matrix hatS\ntwostep::Bool: true if GMM problem was re-optimized with hatW = hatS^-1\nnobs::Int: number of observations T\nnmom::Int: number of moments M\nnpar::Int: number of parameters P","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"info: Info\nFunction summary only displays the Sargan-Hansen test when twostep = true. The behavior of other functions is not affected by twostep.","category":"page"},{"location":"general_gmm/#Available-methods","page":"General GMM models","title":"Available methods","text":"","category":"section"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"The methods below always take a GMM object G as argument, and use the data (f hattheta hatW hatS) stored in it. They never change G in place. See API documentation for additional details.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"sample(G): compute sample of empirical moments f(hattheta).\nmoments(G): compute vector of empirical moments hatg(hattheta).\nvcov(G; lags=0): return new GMM object storing the Newey-West estimator of the long-run covariance matrix hatS, with selected number of lags (see Long-run variance).\njacobian(G): compute the estimate hatD of the Jacobian matrix (dim 1: moments, dim 2: parameters).\noptimize(G; <kwargs>): return new GMM object storing the parameter vector that solves problem (1) with the weighting matrix stored in G (that is, G.weight). Long-run covariance matrix is not recalculated. See info box below. \nreoptimize(G; <kwargs>): return new GMM object storing the parameter vector that solves problem (1) with the efficient weighting matrix S^-1 (that is, inv(G.longcov)). Long-run covariance matrix is not recalculated. See info box below.\ncov(G): compute asymptotic covariance matrix hatV_theta  T of hattheta.\nvar(G): compute element-by-element asymptotic variance of hattheta.\nstd(G): compute element-by-element asymptotic standard deviation of hattheta.\ncor(G): compute asymptotic correlation matrix of hattheta.\nmomcov(G): compute asymptotic covariance matrix hatV_g  T of hattheta.\nsummary(G): print summary of results.\nwald(G; R, r, subset): print result of the Wald test on the null R theta_sub = r, where theta_sub is a subset of theta corresponding to θ[subset].","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"info: Info\nMethods optimize and reoptimize perform numerical optimization of (1) using the optimize function of the Optim package, passing over all keyword arguments (e.g. method=Optim.BFGS()) and using G.params as initial condition for the search.","category":"page"},{"location":"general_gmm/#Long-run-variance","page":"General GMM models","title":"Long-run variance","text":"","category":"section"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"The asymptotic covariance matrix of sample moments hatg(theta_0), also known as long-run variance, is","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"S \n= \nlim_T rightarrow infty T  E left hatg(theta_0) hatg(theta_0) right \n= \nsum_i=-infty^infty E left f(X_t theta_0)  f(X_t-i theta_0) right","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"The package estimates S through the heteroskedasticity and autocorrelation consistent (HAC) Newey-West estimator","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"    hatS = \n    frac1T sum_t=1^T hatf_t hatf_t\n    +\n    frac1T sum_l=1^k sum_t=l+1^T w_l left( hatf_t hatf_t-l + hatf_t-l hatf_t right)","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"where w_l = 1 - l(k+1). Parameter k determines the number of terms with non-zero weight entering the second sum above. When k = 0 (the default of function vcov), the statistic above reduces to the Huber-White estimator.","category":"page"},{"location":"general_gmm/#Computing-the-Jacobian","page":"General GMM models","title":"Computing the Jacobian","text":"","category":"section"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"The estimate hatD = partial hatg(hattheta)partial theta of the Jacobian matrix is critical to compute the standard deviation of estimated parameters hattheta and empirical moments hatg(hattheta). ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Function jacobian uses a simple finite-differences algorithm to approximate partial hatg(hattheta)partial theta. In the direction of m-th component theta_m of the parameter vector:","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"    fracpartial hatg(theta)partial theta_m\n    = \n    frachatg(theta_1dots theta_m+epsilondotstheta_P) - hatg(X (theta_1dots theta_m-epsilondotstheta_P))2 epsilon","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"with 2 epsilon = 1e-8. ","category":"page"},{"location":"general_gmm/#Examples","page":"General GMM models","title":"Examples","text":"","category":"section"},{"location":"general_gmm/#Estimating-the-Mean","page":"General GMM models","title":"Estimating the Mean","text":"","category":"section"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"In this example, we estimate the mean mu = 0 of a Normal distribution. The moment condition is Ex - mu = 0.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"using MethodMoments, Distributions\nimport Random # hide\nRandom.seed!(12) # hide\n\nx = rand(Normal(), 10000) # Normal(0, 1) sample\nf(μ) = x .- μ[1] # sample function\n\nG = optimize(gmm(f, [2.0])); \nsummary(G)","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"If we knew beforehand the optimizing value μ of mu, we could instead skip the optimization step, running only G = gmm(f, [μ]).","category":"page"},{"location":"general_gmm/#Gamma-Distribution","page":"General GMM models","title":"Gamma Distribution","text":"","category":"section"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"We model the data as a sequence of realizations of a Gamma distribution, with shape parameter alpha = 1 and scale parameter eta = 2. ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"using MethodMoments, Distributions, LinearAlgebra, Optim\nimport Random # hide\nRandom.seed!(12) # hide\nx = rand(Gamma(1, 2), 1000) # sample\nnothing # hide","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"We estimate the parameter vector theta = (alpha eta) by matching the first three moments of the Gamma distribution: ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"    EX - mu = 0 \n    qquad \n    E left (X - mu)^2 - sigma^2 right = 0 \n    qquad \n    E left left( fracX - musigma right)^3 - frac2sqrtalpha right = 0 ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"where mu = alpha eta and sigma = sqrtalpha eta^2. These moments define function f in Julia.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"function f(θ)\n    α, η = θ\n\n    # if outside valid range, return large moment error\n    if (α < 0) | (η < 0)\n        return 1e10 * ones(1000, 3)\n    end\n\n    μ = α * η\n    σ = sqrt(α * η^2)\n\n    M1 = x .- μ\n    M2 = M1 .^ 2 .- σ^2\n    M3 = (M1 ./ σ) .^ 3 .- (2 / sqrt(α))\n    return [M1 M2 M3]\nend\nnothing # hide","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"The output of f gives the moment sample. ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"We can pre-specify the weighting matrix to focus on specific moments. For instance, we might be particularly interested in matching the sample variance:","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"# Exogenous weighting\nguess = [5, 5]\nweight = diagm([1, 5, 1])\nG = optimize(gmm(f, guess, weight));\nnothing # hide","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"We suspect that observations are serially correlated. To ensure consistency of our estimates of parameter variance, we change the estimate of the long-run covariance matrix using the Newey-West estimator, with three lags.  ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"G = vcov(G, lags = 3);\nnothing # hide","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"We are ready to check the results of the first-stage estimation.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"summary(G)","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"We can now solve the GMM problem a second time, using our Newey-West estimate of the long-run covariance matrix S^-1 to compute the (feasible) optimal weighting matrix hatS^-1.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"# (Feasible) Optimal weighting \nGo = reoptimize(G, method=Optim.Newton())\nnothing # hide","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"The GMM object Go computed from the call to reoptimize does not change the stored estimate of the long-run variance matrix (Go.longcov == G.longcov returns true). ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Since Go.twostep == true, printed summary now reports the Sargan-Hansen test.","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"summary(Go)","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Parameter estimates in G and Go differ because the optimal weighting matrix raises the penalty on first moment errors: ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"diag(Go.weight) # Optimal weighting matrix","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"Indeed, the two-step solution better matches the sample mean: ","category":"page"},{"location":"general_gmm/","page":"General GMM models","title":"General GMM models","text":"abs.([moments(G)[1] moments(Go)[1]]) # Go better matches first moment condition","category":"page"},{"location":"#MethodMoments.jl","page":"Home","title":"MethodMoments.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"MethodMoments.jl is a Julia package that provides simple tools to handle Generalized Method of Moments (GMM) estimation and linear regression models in a GMM context.","category":"page"},{"location":"","page":"Home","title":"Home","text":"This package is licensed under the MIT License.","category":"page"},{"location":"linear_regression/#Linear-Regressions","page":"Linear regressions","title":"Linear Regressions","text":"","category":"section"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"MethodMoments.jl offers functions to solve univariate linear regression models by ordinary least squares (OLS) or through instrumental variables (IV). The goal of the tutorial below is to quickly present these functions. Hayashi (2000) provides a detailed treatment and the technical assumptions behind the results below. ","category":"page"},{"location":"linear_regression/#Minimal-example","page":"Linear regressions","title":"Minimal example","text":"","category":"section"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"We simulate the regression y_t = c + beta_1 X_1t + beta_2 X_2t + e_t. The true parameters are c=0, beta_1 = beta_2 = 1. First, we estimate by OLS. ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"using MethodMoments \nusing Distributions\nimport Random # hide\nRandom.seed!(1) # hide\nX = randn(100, 2) \nY = X[:,1] + X[:, 2] + randn(100)\nols = reg(Y, X)\nsummary(ols)\nnothing # hide","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Now, we estimate coefficients by IV, adding a new instrument H_t = X_1t + w_t, where w_t is a random shock.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"The weighting matrix must be 4 times 4 (three variable X_1 X_2 N and one constant instrument).","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"N = X[:, 1] + randn(100)\nZ = [X N] # instruments\nW = [1 0 0 0; 0 1 0 0; 0 0 1 0; 0 0 0 1] # weighting matrix\niv = reg(Y, X, Z, W)\nsummary(iv)\nnothing # hide","category":"page"},{"location":"linear_regression/#The-univariate-regression","page":"Linear regressions","title":"The univariate regression","text":"","category":"section"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"The univariate linear model is ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    y_t = beta X_t + e_t ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"where y_t is a scalar (hence \"univariate\"), X_t is a P-sized vector of explanatory variables (or covariates), beta is the true parameter vector, and e_t is the error term with variance \\sigma^2. Let Z_t be a M-sized vector of instruments. The moment condition we apply to estimate beta is","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    E left e_t Z_t right = 0       \n    qquad qquad (1)","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Hence, there are P parameters and M moment conditions. ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"The IV estimator requires the following two conditions to hold.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Order condition: M geq P. If M = P, the model is exactly identified. If M  P, the model is overidentified. If M  P, the model is underidentified and cannot be estimated, as there are multiple solutions to the sample counterpart of (1).\nRank condition: Matrix Eleft Z_t X_t right must be full column rank, i.e., its rank must be P.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Let y_T times 1 be a sample of y_t with Tobservations,  X_T times P a sample of explanatory variables (each row corresponding to one observation X_t), and Z_T times M a sample of instruments. ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Let hatW be the weighting matrix of a GMM problem that estimates beta by targeting moment conditions (1) (see The GMM setup). The estimator of beta that solves that problem, or the GMM estimator is ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    hatbeta =  (XZ  hatW  ZX)^-1  XZ   hatW  Zy qquad qquad (2)","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"If the model is exactly identified, (M = P), (2) becomes the instrumental variables (IV) estimator:","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    hatbeta_IV = (ZX)^-1 Zy qquad qquad (3)","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"If covariates are themselves the instruments (Z_t = X_t), (2) becomes the ordinary least squares (OLS) estimator:","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    hatbeta_OLS = (XX)^-1 Xy qquad qquad (4)","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Both the IV and the OLS estimators perfectly replicate moment conditions (1), in sample. ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"If the weighting matrix is hatW = (ZZ)^-1, (2) becomes the two-stage least squares (2SLS) estimator:","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    hatbeta_2SLS =  (PX) (PX)^-1  (PX) y qquad qquad (5)","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"where P = Z (ZZ)^-1 Z is the projection matrix of Z. The 2SLS estimator (5) is the OLS estimator of a regression of y on the OLS fit of X on Z.","category":"page"},{"location":"linear_regression/#Asymptotic-distributions","page":"Linear regressions","title":"Asymptotic distributions","text":"","category":"section"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Let W = hatW or its probability limit if hatW is a function of the sample.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Let hatS be a consistent estimator of the long-run variance matrix S = lim_T rightarrow infty T  E left hatg(beta) hatg(beta) right (where hatg(b) equiv T^-1 sum_t=1^T (y_t - bX_t) Z_t is the empirical average of the moment function). See section Long-run variance for more details.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Let D = -Z X  T, which consistently estimates D = -EZ_t X_t.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Parameter Distribution: hatbeta oversetpto beta and ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    sqrtT (hatbeta - beta) to mathcalN(0 V_beta) \n    qquad textwhere qquad\n    V_beta = (D W D)^-1 D W S W D (D W D)^-1","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"If W = S^-1 is the optimal weighting matrix, then V_beta = (D S^-1 D)^-1. ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Moments Distribution: hatg(hatbeta) oversetpto 0 and ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    sqrtT hatg(hatbeta) to mathcalN(0 V_g) \n    qquad textwhere qquad\n    V_g = (I - D (D W D)^-1 D W) S (I - D (D W D)^-1 D W)","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"If the model is exactly identified, V_g = 0_M times M.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"If W = S^-1 is the optimal weighting matrix, then V_g = S - D(D S^-1D)^-1 D. ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Note the difference between S and V_g: the former is the asymptotic variance of hatg(beta); the latter is that of hatg(hatbeta). V_g is typically a singular matrix.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"The Sargan-Hansen test for overidentified restrictions, or J test, uses the statistic hatJ = hatg(hattheta) hatW hatg(hattheta). If hatW = hatS^-1, then ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    T  hatJ rightarrow chi^2 (M-P)","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Estimators: The (consistent) estimators hatV_beta of V_beta and hatV_g of V_g computed by GMM.jl simply replace hatW, hatD and hatS in the corresponding formulas.","category":"page"},{"location":"linear_regression/#Running-a-regression","page":"Linear regressions","title":"Running a regression","text":"","category":"section"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Use the constructor function reg to build an object of type Regression, further discussed below. Function reg takes up to four positional arguments: the output vector Y, the regressor array X, the instruments array Z and the weighting matrix W.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"reg(Y, X, Z, W; intercept=true)","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Y should be an AbstractVector. X and Z should be AbstractVecOrMat. Y, X and Z must present observations in rows, variables in columns. W must be a square, positive-definite AbstractMatrix. Weighting matrix must be square, of size size(Z, 2) + intercept. You can omit Z and W.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"To estimate by OLS, run reg(Y, X).\nTo estimate by IV, run reg(Y, X, Z, W).\nIf W is omitted, it defaults to the identity.\nIf intercept = true, reg adds a constant both as a regressor and as an instrument.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Regression objects are not mutable. Its fields are:","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"G::GMM: GMM object corresponding to the orthogonality conditions (1)\nY::Vector: output vector Y\nX::Matrix: right-hand side variables X\nZ::Matrix: instruments Z\nW::Matrix: weighting matrix W\nintercept::Bool: true if constant added\nnobs::Int: number of observations T\nnins::Int: number of instruments (including constant) M\nnrhs::Int: number of right-hand side variables (including intercept) P","category":"page"},{"location":"linear_regression/#Fit-and-model-evaluation","page":"Linear regressions","title":"Fit and model evaluation","text":"","category":"section"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Besides estimators and their asymptotic distribution, GMM.jl computes some additional statistics related to model fit and evaluation.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"The sample error is e_t = y_t - hatbeta X_t. The MLE estimate of the error variance is ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    hatsigma^2_e = frac1T sum_t=1^infty e_t^2","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"The coefficient of determination, or R-squared, is","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    R^2 = 1 - frachatsigma^2_ehatsigma_y^2","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"where hatsigma_y^2 = T^-1 sum_t=1^T (y_t - bary)^2 estimates the unconditional variance of y_t (here, bary is the sample average of y_t). The adjusted R-squared is","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    R^2_adj = 1 - (1 - R^2) times fracT-1T-P","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Assuming errors are Gaussian, the estimated log-likelihood of the model is","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    LLH = -fracT2 log(2 pi) - fracT2 log( hatsigma^2 ) - fracsum_t=1^T e_t^2 2 hatsigma^2","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"The Akaike information criterion is","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    AIC = 2 times P - 2 times LLH","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"The Bayesian information criterion is ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"    BIC = log(T) times P - 2 times LLH","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"AIC and BIC are often used for model selection. ","category":"page"},{"location":"linear_regression/#Available-methods","page":"Linear regressions","title":"Available methods","text":"","category":"section"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"The methods below always take a Regression object re as argument, and never change re in place. Many of these methods derive from the GMM methods applied to the GMM object re.G. See API documentation for details.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"intercept(re): get intercept.\ncoef(re): get estimated coefficients (excluding intercept).\nvcov(re; lags=0): return new Regression object storing the Newey-West estimator of the long-run covariance matrix hatS, with selected number of lags, in its GMM field  (see Long-run variance). Use this method to change estimates of coefficients' variance and covariance. When lags=0, we get White standard errors.\ncov(re): get covariance matrix of coefficients V_beta  T, excluding the intercept term. (If you are interested in the variance of the intercept term, run cov(re.G)). \nvar(re): get element-by-element variance of coefficients, excluding the intercept term. Use std(re) for standard errors.\ncor(re): get correlation matrix of coefficients, excluding the intercept term\nfit(re, X): fit X beta using estimated coefficients. If X is omitted, apply to sample used for estimation re.X.\ner(re): sample of fitted errors (dim 1: moments, dim 2: parameters).\ner_var(re): get maximum likelihood estimate of error variance sigma^2 (no correction for lost degrees of freedom). Use er_std to get standard deviation sigma.\nr2(re, adjust=false): get coefficient of determination R^2, adjusted or not.\nllh(re): get log-likelihood LLH.\naic(re): get Akaike information criterion AIC.\nbic(re): get Bayesian information criterion BIC.\nsummary(re): print summary of results.\nwald(re; R, r, subset): print result of the Wald test on the null R beta_sub = r, where beta_sub is a subset of beta corresponding to β[subset], excluding the intercept.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"info: Info\nCalling vcov(re) changes the long-term covariance matrix stored in the GMM object re.G, so you don't need to change it manually.","category":"page"},{"location":"linear_regression/#Examples","page":"Linear regressions","title":"Examples","text":"","category":"section"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"We return to the example at the top of the page. Object ols stores the results for the regression y_t = c + beta_1 X_1t + beta_2 X_2t + e_t. For instance, coefficients and White standard deviations:","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"println(\"OLS estimates: $(coef(ols))\")\nprintln(\"(standard deviation): $(std(ols))\")\nnothing # hide","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"To change variance estimates to the Newey-West formula with three lags:","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"ols = vcov(ols, lags=3)\nprintln(\"(standard deviation): $(std(ols))\")\nnothing # hide","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Intercept:","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"println(\"Intercept: $(intercept(ols))\")\nnothing # hide","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Field ols.G stores the GMM object referring to the orthogonality conditions that define OLS estimate. It contains all relevant data for the problem. For instance, ols.G.params returns coefficient estimates, including the intercept. ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"println(\"All params: $(ols.G.params)\")\nnothing # hide","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Therefore, to access the standard deviation of the intercept term, you can call std(ols.G)[1]. (When the model involves an intercept, it is always ordered first in the GMM problem.)","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Given an array of right-hand side regressors, we can fit it using estimated coefficients: ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"new_X = randn(50, 2) # no need to add intercept\nfit(ols, new_X)\nnothing # hide","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Omitting the second argument, fit(ols) returns the fit of the sample X stored in the regression object (used for estimation) ols.X.","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"To access the sample of fitted errors, run er(ols). The estimate of the variance of these errors sigma^2 is:","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"println(\"Error variance: $(er_var(ols))\")\nnothing # hide","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"We can access the following model evaluation metrics:","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"println(\"R-squared: $(r2(ols))\")\nprintln(\"Akaike criterion: $(aic(ols))\")\nprintln(\"Bayesian criterion: $(bic(ols))\")\nnothing # hide","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"To compute a summary of the model, use method summary, as in the top of this page. ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Finally, we can perform a Wald test of the null hypothesis R beta_subset = r. In the example below, we test beta_1 + beta_2 = 2 and beta_1 = 1.  ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"R = [1 1; 1 0]\nr = [2, 1]\nsubset = [1, 2]\nwald(ols, R, r, subset)\nnothing # hide","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"Keyword argument subset of wald does not consider the intercept. Therefore, subset = [1] would refer to the first angular coefficient coef(ols)[1], not intercept(ols). ","category":"page"},{"location":"linear_regression/","page":"Linear regressions","title":"Linear regressions","text":"If your test involves the intercept term, use method wald applied to underlying GMM object (which includes the intercept term): wald(ols.G, R, r, subset).","category":"page"}]
}
